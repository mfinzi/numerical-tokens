{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_datasets import NLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({(3, 7), (5, 7), (0, 2), (0, 5), (1, 6), (0, 8), (2, 5), (1, 3), (2, 8), (6, 8), (5, 6), (3, 6), (0, 1), (0, 7), (1, 2), (2, 7), (1, 5), (1, 8), (6, 7), (3, 5), (3, 8), (5, 8), (0, 3), (0, 6), (2, 3), (1, 7), (2, 6), (7, 8)}, ('Determine if there is a path between two nodes in the graph. Note that (i,j) means that node i and node j are connected with an undirected edge.\\nGraph', '\\nQ: Is there a path between node 8 and node 2?\\nA:'), 'The answer is yes.')\n"
     ]
    }
   ],
   "source": [
    "test_dataset = NLGraph(root=\"./NLGraph\", task=\"connectivity\", split=\"test\")\n",
    "# test_dataset = NLGraph(root=\"./NLGraph\", task=\"cycle\", split=\"test\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/locus/project_data/project_data2/dylansam/Mistral-7B-Instruct-v0.2\"\n",
    "# model_path = \"/data/locus/project_data/project_data2/dylansam/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# model_path = \"/data/datasets/models/huggingface/meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_path = \"/data/datasets/models/huggingface/meta-llama/Llama-2-70b-chat-hf\"\n",
    "# model_path = \"/data/locus/project_data/project_data2/dylansam/Llama-2-7b-hf\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try purely naive tokenization (w LLaMA tokenizer) for classification tasks: connectivity and cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 415, 4372, 349, 5081]\n"
     ]
    }
   ],
   "source": [
    "print(base_tokenizer.encode(\"The answer is yes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine if there is a path between two nodes in the graph. Note that (i,j) means that node i and node j are connected with an undirected edge.\n",
      "Graph: (0,8) (0,1) (0,6) (0,2) (0,3) (0,7) (0,5) (1,8) (1,6) (1,2) (1,3) (1,7) (1,5) (2,8) (2,6) (2,3) (2,7) (2,5) (3,8) (3,6) (3,7) (3,5) (5,8) (5,6) (5,7) (6,8) (6,7) (7,8)\n",
      "Q: Is there a path between node 8 and node 2?\n",
      "A:\n",
      "The answer is \n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.string_data[0])\n",
    "print(test_dataset.labels[0][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371/371 [00:38<00:00,  9.65it/s]\n"
     ]
    }
   ],
   "source": [
    "zs_accuracy = 0\n",
    "\n",
    "answers = []\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_dataset)), total=len(test_dataset)):\n",
    "    string_question = test_dataset.string_data[i]\n",
    "    answer = test_dataset[i][2].lower()\n",
    "    bool_answer = 1 if \"yes\" in answer else 0\n",
    "\n",
    "    # input_prompt = string_question + \" \" + \n",
    "    input_prompt = string_question + \" The answer is\"\n",
    "    inputs = base_tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs).logits\n",
    "\n",
    "    # get index of \"yes\" or \"no\" answer\n",
    "    logits = logits[0, -1, :]\n",
    "    yes_token = base_tokenizer.encode(\"yes\")[1]\n",
    "    no_token = base_tokenizer.encode(\"no\")[1]\n",
    "    prob = torch.stack([logits[no_token], logits[yes_token]])\n",
    "    prob = torch.nn.functional.softmax(prob, dim=0)\n",
    "    pred = torch.argmax(prob).item()\n",
    "\n",
    "    preds.append(pred)\n",
    "    answers.append(bool_answer)\n",
    "\n",
    "    if pred == bool_answer:\n",
    "        zs_accuracy += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZS Accuracy, naive string input 0.6522911051212938\n"
     ]
    }
   ],
   "source": [
    "print(\"ZS Accuracy, naive string input\", zs_accuracy / len(test_dataset))\n",
    "\n",
    "# connectivity ~65%\n",
    "# cycle ~50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 33, and the edges are: (24,2) (17,26) (1,8) (30,9) (28,31) (3,31) (25,28) (6,9) (28,4) (0,33) (31,10) (23,32) (12,13) (6,16) (15,14) (24,11) (19,21) (24,20) (12,3) (0,24) (13,22) (19,7) (17,23) (29,18) (28,14) (0,17) (5,27) (26,18) (28,2) (8,30) (16,17) (33,21) (21,5)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: No, there is no cycle in this graph.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NLGraph(root=\"./NLGraph\", task=\"connectivity\", split=\"train\")\n",
    "train_dataset = NLGraph(root=\"./NLGraph\", task=\"cycle\", split=\"train\")\n",
    "print(train_dataset.string_data[0], train_dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_examples = \"\"\n",
    "for i in range(2):\n",
    "    answer = train_dataset[i][2].lower()\n",
    "    answer = 1 if \"yes\" in answer else 0\n",
    "    if answer == 1:\n",
    "        context_examples += train_dataset.string_data[i] + \" The answer is yes.\"\n",
    "    else:\n",
    "        context_examples += train_dataset.string_data[i] + \" The answer is no.\"\n",
    "    \n",
    "    context_examples += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 33, and the edges are: (24,2) (17,26) (1,8) (30,9) (28,31) (3,31) (25,28) (6,9) (28,4) (0,33) (31,10) (23,32) (12,13) (6,16) (15,14) (24,11) (19,21) (24,20) (12,3) (0,24) (13,22) (19,7) (17,23) (29,18) (28,14) (0,17) (5,27) (26,18) (28,2) (8,30) (16,17) (33,21) (21,5)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: The answer is no.\n",
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 24, and the edges are: (22,9) (21,16) (13,7) (18,0) (16,22) (17,7) (3,7) (4,17) (8,3) (2,16) (1,13) (14,9) (24,12) (0,15) (5,3) (10,5) (18,19) (9,23) (5,24) (6,22) (11,12) (12,20) (22,20) (0,3)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: The answer is no.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 26/371 [00:04<01:05,  5.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# get index of \"yes\" or \"no\" answer\u001b[39;00m\n\u001b[1;32m     20\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:673\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    670\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mupdate(key_states, value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx, cache_kwargs)\n\u001b[1;32m    672\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 673\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;241m1\u001b[39m, q_len, kv_seq_len):\n",
      "File \u001b[0;32m~/miniconda3/envs/tokens/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:191\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[0;32m--> 191\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mreshape(batch, num_key_value_heads \u001b[38;5;241m*\u001b[39m n_rep, slen, head_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "icl_accuracy = 0\n",
    "\n",
    "answers = []\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_dataset)), total=len(test_dataset)):\n",
    "\n",
    "    string_question = test_dataset.string_data[i]\n",
    "    answer = test_dataset[i][2].lower()\n",
    "    bool_answer = 1 if \"yes\" in answer else 0\n",
    "\n",
    "    input_prompt = context_examples + string_question + \" The answer is\"\n",
    "    inputs = base_tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs).logits\n",
    "\n",
    "    # get index of \"yes\" or \"no\" answer\n",
    "    logits = logits[0, -1, :]\n",
    "    yes_token = base_tokenizer.encode(\"yes\")[1]\n",
    "    no_token = base_tokenizer.encode(\"no\")[1]\n",
    "    prob = torch.stack([logits[no_token], logits[yes_token]])\n",
    "    prob = torch.nn.functional.softmax(prob, dim=0)\n",
    "    pred = torch.argmax(prob).item()\n",
    "\n",
    "    preds.append(pred)\n",
    "    answers.append(bool_answer)\n",
    "\n",
    "    if pred == bool_answer:\n",
    "        icl_accuracy += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ZS Ability for Shortest Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NLGraph(root=\"./NLGraph\", task=\"shortest_path\", split=\"test\", string_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_graph(string):\n",
    "    string = string.split(\",\")\n",
    "    \n",
    "    # parsing nodes\n",
    "    nodes = string[1]\n",
    "    nodes = nodes.split(\" \")\n",
    "    node_start = nodes[-3]\n",
    "    node_end = nodes[-1]\n",
    "    nodes = list(range(int(node_start), int(node_end) + 1))\n",
    "\n",
    "    # parsing edges\n",
    "    edges = set()\n",
    "    for s in string[2:-2]:\n",
    "        s = s.split(\" \")\n",
    "        node_0 = int(s[-7])\n",
    "        node_1 = int(s[-4])\n",
    "        weight = int(s[-1])\n",
    "        edges.add((node_0, node_1, weight)) # node 0, node 1, edge weight\n",
    "    return nodes, edges\n",
    "\n",
    "def parse_answer(string):\n",
    "    answer_split = string.split(\" \")\n",
    "    path = answer_split[10]\n",
    "    path = path.split(\",\")\n",
    "    path = [int(p) for p in path]\n",
    "    weight = int(answer_split[-1])\n",
    "    return path, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4, 12, 10, 2], 12)\n"
     ]
    }
   ],
   "source": [
    "graph_string = test_dataset[0][0]\n",
    "answer_string = test_dataset[0][2]\n",
    "\n",
    "# print(graph_string)\n",
    "# print(answer_string)\n",
    "# print(answer_string[: answer_string.find(\"is\") + 3])\n",
    "# print(convert_string_to_graph(graph_string))\n",
    "print(parse_answer(answer_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 2] 7\n",
      "[3, 1, 5, 8] 7\n",
      "[8, 9, 7, 3, 6, 2] 22\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_sp_output(output):\n",
    "    '''\n",
    "    Function to parse LLM output for shortest path\n",
    "    '''\n",
    "    \n",
    "    # Split the output to separate the path from the weight description\n",
    "    # path_parts = re.split(r' with a weight of |\\. The total weight of this path is ', output)\n",
    "    parts = re.split('weight', output)\n",
    "    path_string = parts[0]\n",
    "    weight_string = parts[1]\n",
    "\n",
    "    # Extract the path as numbers\n",
    "    path = re.findall(r'\\d+', path_string)\n",
    "    path = list(map(int, path))\n",
    "\n",
    "    # Extract the weight as an integer if present (take the last one?)\n",
    "    weight = re.findall(r'\\d+', weight_string)\n",
    "    if weight:\n",
    "        weight = int(weight[0])\n",
    "\n",
    "        return path, weight\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Test the corrected function with the original output examples\n",
    "\n",
    "# Example usage\n",
    "outputs = [\n",
    "    \"7 -> 1 -> 2 with a weight of 7.\",\n",
    "    \"3-1-5-8 with a weight of 7.\",\n",
    "    \"8-9-7-3-6-2. The total weight of this path is 22.\"\n",
    "]\n",
    "\n",
    "for output in outputs:\n",
    "    path, weight = parse_sp_output(output)\n",
    "    print(path, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]/home/dylansam/miniconda3/envs/tokens/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 1/64 [00:01<01:33,  1.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 2/64 [00:02<01:18,  1.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▍         | 3/64 [00:03<01:20,  1.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▋         | 4/64 [00:05<01:19,  1.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 5/64 [00:06<01:19,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 6/64 [00:07<01:17,  1.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█         | 7/64 [00:09<01:16,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▎        | 8/64 [00:10<01:15,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 9/64 [00:12<01:13,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 10/64 [00:13<01:13,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 11/64 [00:14<01:12,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 12/64 [00:16<01:11,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 edges long and has the following sequence of nodes: 5 -> 1 -> 3. The weight of this path is the sum of the weights of its edges, which is 4 +\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13/64 [00:17<01:09,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 units long and goes through nodes 0, 3, and 6.\n",
      "Explanation:\n",
      "To find the shortest path from node 5 to node 6, we can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/64 [00:18<01:08,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 15/64 [00:20<01:06,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 16/64 [00:21<01:05,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 17/64 [00:23<01:04,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 18/64 [00:24<01:03,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|██▉       | 19/64 [00:25<01:02,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███▏      | 20/64 [00:27<01:00,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 21/64 [00:28<00:58,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 22/64 [00:29<00:56,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 23/64 [00:31<00:54,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 24/64 [00:32<00:53,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▉      | 25/64 [00:33<00:52,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 26/64 [00:35<00:51,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 27/64 [00:36<00:50,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 28/64 [00:37<00:48,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 29/64 [00:39<00:47,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 30/64 [00:40<00:46,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 31/64 [00:42<00:45,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 32/64 [00:43<00:44,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 33/64 [00:44<00:42,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 34/64 [00:46<00:41,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▍    | 35/64 [00:47<00:39,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▋    | 36/64 [00:48<00:38,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 37/64 [00:50<00:37,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 38/64 [00:51<00:35,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████    | 39/64 [00:53<00:34,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▎   | 40/64 [00:54<00:32,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 edges long. One possible path is: node 5 -> node 3 -> node 4. The weight of this path is the sum of the weights of its edges, which is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 41/64 [00:55<00:31,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 42/64 [00:57<00:29,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 43/64 [00:58<00:28,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 44/64 [00:59<00:27,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 45/64 [01:01<00:26,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 46/64 [01:02<00:24,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 47/64 [01:04<00:23,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 48/64 [01:05<00:21,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 49/64 [01:06<00:20,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 50/64 [01:08<00:19,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|███████▉  | 51/64 [01:09<00:17,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████▏ | 52/64 [01:10<00:16,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 53/64 [01:12<00:14,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 54/64 [01:13<00:13,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 55/64 [01:14<00:12,  1.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 56/64 [01:16<00:10,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▉ | 57/64 [01:17<00:08,  1.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 58/64 [01:18<00:07,  1.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 59/64 [01:20<00:06,  1.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 60/64 [01:21<00:05,  1.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 61/64 [01:22<00:04,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 62/64 [01:24<00:02,  1.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 63/64 [01:25<00:01,  1.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 64/64 [01:26<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZS Accuracy, shortest path 0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zs_accuracy = 0\n",
    "count = 0\n",
    "\n",
    "preds = []\n",
    "actual_labels = []\n",
    "\n",
    "for i in tqdm(range(len(test_dataset)), total=len(test_dataset)):\n",
    "\n",
    "    graph_string = test_dataset[i][0]\n",
    "    answer_string = test_dataset[i][2]\n",
    "\n",
    "    nodes, edges = convert_string_to_graph(graph_string)\n",
    "    path, weight = parse_answer(answer_string)\n",
    "\n",
    "    input_prompt = graph_string + \" \" + answer_string[: answer_string.find(\"is\") + 3]\n",
    "    inputs = base_tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # generate 20 tokens\n",
    "        outputs = model.generate(inputs, max_length=inputs.shape[1] + 40, do_sample=False, temperature=0.0)\n",
    "    \n",
    "    # show decoded output\n",
    "    decoded_output = base_tokenizer.decode(outputs[0][inputs.shape[1]:])\n",
    "    try:\n",
    "        llm_path, llm_weight = parse_sp_output(decoded_output)\n",
    "    except:\n",
    "        llm_path, llm_weight = None, None\n",
    "    \n",
    "    # print(\"actual\", path, weight)\n",
    "    # print(\"pred\", llm_path, llm_weight)\n",
    "\n",
    "    if llm_path == None:\n",
    "        print(decoded_output)\n",
    "\n",
    "    else:\n",
    "        # break\n",
    "        if path == llm_path and weight == llm_weight:\n",
    "            zs_accuracy += 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    preds.append((llm_path, llm_weight))\n",
    "    actual_labels.append((path, weight))\n",
    "    # if count == 10:\n",
    "        # break\n",
    "\n",
    "print(\"ZS Accuracy, shortest path\", zs_accuracy / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4, 12, 10, 2], 12) ([4, 12, 3, 2], 5)\n",
      "([11, 12, 4, 3, 9, 17, 7], 33) ([11, 12, 8, 7], 10)\n",
      "([7, 1, 9, 3, 2], 12) ([7, 1, 2], 7)\n",
      "([7, 8, 3], 3) ([7, 5, 3], 4)\n",
      "([3, 5, 8], 4) ([3, 1, 5, 8], 7)\n",
      "([2, 0, 4], 5) ([2, 1, 4], 3)\n",
      "([11, 8, 9, 7, 0, 2], 31) ([8, 9, 7, 3, 6, 2], 22)\n",
      "([1, 2, 3], 5) ([1, 2, 3], 2)\n",
      "([0, 3, 4], 4) ([0, 1, 2, 4], 3)\n",
      "([6, 1, 4], 3) ([6, 3, 4], 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(actual_labels[i], preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLGraph(root=\"./NLGraph\", task=\"shortest_path\", split=\"train\", string_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an undirected graph, the nodes are numbered from 0 to 4, and the edges are:\n",
      "an edge between node 0 and node 1 with weight 4,\n",
      "an edge between node 0 and node 3 with weight 4,\n",
      "an edge between node 0 and node 2 with weight 3,\n",
      "an edge between node 0 and node 4 with weight 1,\n",
      "an edge between node 1 and node 3 with weight 3,\n",
      "an edge between node 1 and node 4 with weight 1,\n",
      "an edge between node 2 and node 3 with weight 3,\n",
      "an edge between node 2 and node 4 with weight 2,\n",
      "an edge between node 3 and node 4 with weight 1.\n",
      "Q: Give the shortest path from node 1 to node 2.\n",
      "A: The shortest path from node 1 to node 2 is 1,4,2 with a total weight of 3\n",
      "In an undirected graph, the nodes are numbered from 0 to 16, and the edges are:\n",
      "an edge between node 0 and node 3 with weight 4,\n",
      "an edge between node 0 and node 13 with weight 7,\n",
      "an edge between node 0 and node 14 with weight 8,\n",
      "an edge between node 0 and node 6 with weight 5,\n",
      "an edge between node 1 and node 16 with weight 5,\n",
      "an edge between node 1 and node 12 with weight 2,\n",
      "an edge between node 1 and node 11 with weight 9,\n",
      "an edge between node 2 and node 4 with weight 9,\n",
      "an edge between node 2 and node 3 with weight 9,\n",
      "an edge between node 2 and node 6 with weight 8,\n",
      "an edge between node 3 and node 16 with weight 4,\n",
      "an edge between node 3 and node 10 with weight 8,\n",
      "an edge between node 3 and node 13 with weight 1,\n",
      "an edge between node 3 and node 6 with weight 9,\n",
      "an edge between node 4 and node 11 with weight 2,\n",
      "an edge between node 4 and node 15 with weight 2,\n",
      "an edge between node 4 and node 8 with weight 10,\n",
      "an edge between node 5 and node 16 with weight 9,\n",
      "an edge between node 5 and node 15 with weight 5,\n",
      "an edge between node 6 and node 9 with weight 3,\n",
      "an edge between node 6 and node 10 with weight 4,\n",
      "an edge between node 6 and node 7 with weight 6,\n",
      "an edge between node 7 and node 10 with weight 6,\n",
      "an edge between node 7 and node 13 with weight 10,\n",
      "an edge between node 8 and node 11 with weight 3,\n",
      "an edge between node 9 and node 15 with weight 3,\n",
      "an edge between node 10 and node 12 with weight 4,\n",
      "an edge between node 11 and node 16 with weight 9,\n",
      "an edge between node 14 and node 16 with weight 3.\n",
      "Q: Give the shortest path from node 9 to node 1.\n",
      "A: The shortest path from node 9 to node 1 is 9,6,10,12,1 with a total weight of 13\n",
      "In an undirected graph, the nodes are numbered from 0 to 11, and the edges are:\n",
      "an edge between node 0 and node 3 with weight 9,\n",
      "an edge between node 0 and node 9 with weight 1,\n",
      "an edge between node 0 and node 10 with weight 7,\n",
      "an edge between node 0 and node 4 with weight 9,\n",
      "an edge between node 1 and node 2 with weight 3,\n",
      "an edge between node 1 and node 7 with weight 3,\n",
      "an edge between node 2 and node 3 with weight 3,\n",
      "an edge between node 2 and node 9 with weight 6,\n",
      "an edge between node 2 and node 10 with weight 6,\n",
      "an edge between node 2 and node 7 with weight 7,\n",
      "an edge between node 2 and node 8 with weight 1,\n",
      "an edge between node 3 and node 7 with weight 7,\n",
      "an edge between node 4 and node 9 with weight 10,\n",
      "an edge between node 5 and node 7 with weight 2,\n",
      "an edge between node 5 and node 11 with weight 9,\n",
      "an edge between node 6 and node 7 with weight 6,\n",
      "an edge between node 7 and node 8 with weight 5,\n",
      "an edge between node 9 and node 11 with weight 7.\n",
      "Q: Give the shortest path from node 11 to node 10.\n",
      "A: The shortest path from node 11 to node 10 is 11,9,0,10 with a total weight of 15\n",
      "In an undirected graph, the nodes are numbered from 0 to 12, and the edges are:\n",
      "an edge between node 0 and node 7 with weight 7,\n",
      "an edge between node 0 and node 3 with weight 2,\n",
      "an edge between node 0 and node 8 with weight 10,\n",
      "an edge between node 1 and node 2 with weight 2,\n",
      "an edge between node 1 and node 6 with weight 9,\n",
      "an edge between node 2 and node 5 with weight 9,\n",
      "an edge between node 2 and node 11 with weight 2,\n",
      "an edge between node 2 and node 6 with weight 2,\n",
      "an edge between node 3 and node 7 with weight 3,\n",
      "an edge between node 3 and node 5 with weight 2,\n",
      "an edge between node 3 and node 11 with weight 9,\n",
      "an edge between node 4 and node 7 with weight 2,\n",
      "an edge between node 4 and node 9 with weight 9,\n",
      "an edge between node 4 and node 5 with weight 6,\n",
      "an edge between node 5 and node 9 with weight 8,\n",
      "an edge between node 6 and node 10 with weight 10,\n",
      "an edge between node 7 and node 8 with weight 4,\n",
      "an edge between node 8 and node 9 with weight 8,\n",
      "an edge between node 11 and node 12 with weight 8.\n",
      "Q: Give the shortest path from node 3 to node 10.\n",
      "A: The shortest path from node 3 to node 10 is 3,5,2,6,10 with a total weight of 23\n",
      "In an undirected graph, the nodes are numbered from 0 to 17, and the edges are:\n",
      "an edge between node 0 and node 16 with weight 2,\n",
      "an edge between node 0 and node 12 with weight 5,\n",
      "an edge between node 0 and node 2 with weight 7,\n",
      "an edge between node 1 and node 3 with weight 9,\n",
      "an edge between node 1 and node 8 with weight 9,\n",
      "an edge between node 1 and node 13 with weight 5,\n",
      "an edge between node 2 and node 3 with weight 6,\n",
      "an edge between node 2 and node 12 with weight 2,\n",
      "an edge between node 2 and node 15 with weight 10,\n",
      "an edge between node 2 and node 4 with weight 9,\n",
      "an edge between node 3 and node 6 with weight 2,\n",
      "an edge between node 3 and node 17 with weight 2,\n",
      "an edge between node 3 and node 10 with weight 7,\n",
      "an edge between node 3 and node 15 with weight 6,\n",
      "an edge between node 3 and node 13 with weight 7,\n",
      "an edge between node 4 and node 6 with weight 4,\n",
      "an edge between node 4 and node 13 with weight 3,\n",
      "an edge between node 5 and node 6 with weight 9,\n",
      "an edge between node 5 and node 14 with weight 8,\n",
      "an edge between node 7 and node 11 with weight 10,\n",
      "an edge between node 8 and node 12 with weight 6,\n",
      "an edge between node 8 and node 15 with weight 3,\n",
      "an edge between node 8 and node 13 with weight 10,\n",
      "an edge between node 9 and node 14 with weight 9,\n",
      "an edge between node 10 and node 15 with weight 6,\n",
      "an edge between node 11 and node 12 with weight 5,\n",
      "an edge between node 11 and node 17 with weight 9,\n",
      "an edge between node 12 and node 13 with weight 5,\n",
      "an edge between node 15 and node 17 with weight 7.\n",
      "Q: Give the shortest path from node 11 to node 9.\n",
      "A: The shortest path from node 11 to node 9 is 11,17,3,6,5,14,9 with a total weight of 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_examples = \"\"\n",
    "for i in range(5):\n",
    "    context_examples += train_dataset[i][0] + \" \" + train_dataset[i][2] + \"\\n\"\n",
    "\n",
    "print(context_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 1/64 [00:01<01:27,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 2/64 [00:02<01:28,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▍         | 3/64 [00:04<01:25,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▋         | 4/64 [00:05<01:23,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 5/64 [00:07<01:22,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 6/64 [00:08<01:20,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|█         | 7/64 [00:09<01:20,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▎        | 8/64 [00:11<01:18,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 9/64 [00:12<01:16,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 10/64 [00:13<01:15,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 17%|█▋        | 11/64 [00:15<01:14,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|█▉        | 12/64 [00:16<01:13,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 13/64 [00:18<01:11,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 14/64 [00:19<01:10,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|██▎       | 15/64 [00:20<01:07,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 16/64 [00:22<01:07,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|██▋       | 17/64 [00:23<01:06,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 18/64 [00:25<01:04,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|██▉       | 19/64 [00:26<01:04,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|███▏      | 20/64 [00:28<01:02,  1.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|███▎      | 21/64 [00:29<01:00,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 22/64 [00:30<00:58,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 23/64 [00:32<00:56,  1.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 24/64 [00:33<00:55,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|███▉      | 25/64 [00:34<00:53,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 26/64 [00:36<00:52,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 27/64 [00:37<00:51,  1.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 28/64 [00:39<00:49,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 29/64 [00:40<00:48,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 30/64 [00:41<00:47,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 31/64 [00:43<00:46,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 32/64 [00:44<00:44,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 33/64 [00:46<00:43,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 34/64 [00:47<00:42,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▍    | 35/64 [00:48<00:40,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▋    | 36/64 [00:50<00:39,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 37/64 [00:51<00:37,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 38/64 [00:53<00:36,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████    | 39/64 [00:54<00:34,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▎   | 40/64 [00:55<00:33,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▍   | 41/64 [00:57<00:31,  1.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 42/64 [00:58<00:30,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|██████▋   | 43/64 [01:00<00:29,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|██████▉   | 44/64 [01:01<00:28,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 45/64 [01:02<00:26,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 46/64 [01:04<00:25,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████▎  | 47/64 [01:05<00:24,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 48/64 [01:07<00:22,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|███████▋  | 49/64 [01:08<00:21,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 50/64 [01:10<00:19,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|███████▉  | 51/64 [01:11<00:18,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|████████▏ | 52/64 [01:12<00:17,  1.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 83%|████████▎ | 53/64 [01:14<00:15,  1.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 54/64 [01:15<00:14,  1.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 55/64 [01:17<00:12,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 56/64 [01:18<00:11,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|████████▉ | 57/64 [01:19<00:09,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 58/64 [01:21<00:08,  1.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 59/64 [01:22<00:07,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 60/64 [01:24<00:05,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 61/64 [01:25<00:04,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 62/64 [01:26<00:02,  1.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 63/64 [01:28<00:01,  1.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 64/64 [01:29<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL Accuracy, shortest path 0.0078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "icl_accuracy = 0\n",
    "\n",
    "for i in tqdm(range(len(test_dataset)), total=len(test_dataset)):\n",
    "\n",
    "    graph_string = test_dataset[i][0]\n",
    "    answer_string = test_dataset[i][2]\n",
    "\n",
    "    nodes, edges = convert_string_to_graph(graph_string)\n",
    "    path, weight = parse_answer(answer_string)\n",
    "\n",
    "    input_prompt = context_examples + graph_string + \" \" + answer_string[: answer_string.find(\"is\") + 3]\n",
    "    inputs = base_tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # generate 20 tokens\n",
    "        outputs = model.generate(inputs, max_length=inputs.shape[1] + 30, do_sample=False, temperature=0.0)\n",
    "    \n",
    "    # show decoded output\n",
    "    decoded_output = base_tokenizer.decode(outputs[0][inputs.shape[1]:])\n",
    "    # print(\"actual output\", answer_string)\n",
    "    # print(\"pred output\", decoded_output)\n",
    "    \n",
    "    try:\n",
    "        llm_path, llm_weight = parse_sp_output(decoded_output)\n",
    "    except:\n",
    "        llm_path, llm_weight = None, None\n",
    "    \n",
    "    # print(\"actual\", path, weight)\n",
    "    # print(\"pred\", llm_path, llm_weight)\n",
    "\n",
    "    if llm_path == None:\n",
    "        print(decoded_output)\n",
    "    else:\n",
    "        # break\n",
    "        if path == llm_path and weight == llm_weight:\n",
    "            icl_accuracy += 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    preds.append((llm_path, llm_weight))\n",
    "    actual_labels.append((path, weight))\n",
    "    if count == 10:\n",
    "        break\n",
    "    \n",
    "print(\"ICL Accuracy, shortest path\", icl_accuracy / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
